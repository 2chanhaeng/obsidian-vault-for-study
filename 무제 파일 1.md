1. 프로젝트 구현 과정에서 POST 메소드와 쿠키를 어떻게 분석하고 처리했는지 구체적으로 설명해주실 수 있나요?
   대답: 해당 페이지를 브라우저에서 열어서 개발자 도구의 네트워크 탭을 열고 검색을 해봤습니다. 대부분의 검색 앱은 URL 파라미터에 검색어를 담아 GET 요청을 하는데 이 페이지는 특이하게 키워드를 본문에 담은 POST 요청을 보내고 있었습니다. 해당 요청에 있던 본문을 그대로 복사해 Python으로 요청해봤습니다. 그런데 크롤링 제한이라고 써진 페이지를 응답으로 받았습니다. 문제점을 찾아보니 헤더까지 복사해 요청을 보내니 제대로된 결과를 받을 수 있었습니다. 아무래도 쿠키를 이용해 세션을 유지하면서 동시에 이를 이용해 크롤링 여부를 판단하는 것 같았습니다. 그래서 바로 데이터를 요청하는게 아니라 먼저 검색 페이지에 요청을 보내 `Set-Cookie` 헤더를 가져와 파싱해서 쿠키를 가져온 다음 이를 `Cookie` 헤더에 다시 담아 보내는 방식으로 요청을 보내봤습니다. 그런데 여전히 크롤링 제한 응답을 받았습니다. 왜 그런가 싶어서 다시 요청을 비교해보니, 브라우저의 요청에는 `Set-Cookie` 에 없던 쿠키가 추가되어 있었습니다. 문제를 해결하기 위해 페이지의 소스 코드를 열어보았습니다. 코드는 많았지만 대부분의 코드는 jQuery 번들러 코드였고, 해당 페이지에서 직접 사용하는 소스 코드는 많지 않았습니다. 해당 소스 코드에서 쿠키명을 검색하니 페이지 html 코드에서 특정 요소에 담긴 값을 해당 쿠키명으로 쿠키에 추가하는 코드가 있었습니다. 서버에서 레이아웃을 보낼 때, 이 숨겨진 요소를 담은 뒤 해당 요소를 브라우저에서 쿠키로 추가하는 방식으로 크롤링을 방지하고 있었던 것으로 이해했습니다. 그래서 마찬가지로 처음 검색 페이지 응답에서 html 코드도 파싱해 해당 요소에서 숨겨진 쿠키값을 찾아낸 뒤 쿠키로 추가해 크롤링 방지 기법을 우회하고 성공적으로 원하는 값만 빠르게 가져올 수 있도록 프로그램을 완성했습니다.
2. 크롤링을 방지하는 요소들이 많았을 텐데, 이를 우회하기 위해 어떤 기법을 사용했나요?
   1번의 대답으로 대체
3. 크롤러가 정확한 데이터를 수집할 수 있도록 어떤 검증 과정을 거쳤나요?
   요청하는 데이터를 보니, 이름이 중복되거나 저는 입력하지 않았던 항목이 다같이 보내지고 있었습니다. 
4. **구현한 크롤러의 성능을 최적화하기 위해 어떤 기술적 접근을 하셨나요?**
5. **구현한 크롤러는 얼마나 많은 데이터를 처리할 수 있으며, 시간당 처리량이나 성공률은 어떻게 되나요?**
6. **Google Colab을 선택한 이유는 무엇인가요? 다른 배포 방법과 비교했을 때 Google Colab의 장점은 무엇이었나요?**
7. **이 프로젝트에서 Python을 선택하신 이유는 무엇인가요? Node.js와 비교했을 때 어떤 차이점이 있었나요?**
8. **이 크롤러 프로젝트가 비즈니스적으로 어떤 영향을 미쳤나요?**
9. **크롤링 방지 로직이 업데이트되면 이를 어떻게 대처하고 유지 보수할 계획인가요?**
10. **이 프로젝트를 통해 가장 어려웠던 점은 무엇이었고, 이를 어떻게 해결했나요?**