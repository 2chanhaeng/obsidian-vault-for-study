1. 프로젝트 구현 과정에서 POST 메소드와 쿠키를 어떻게 분석하고 처리했는지 구체적으로 설명해주실 수 있나요?
   대답: 해당 페이지를 브라우저에서 열어서 개발자 도구의 네트워크 탭을 열고 검색을 해봤습니다. 대부분의 검색 앱은 URL 파라미터에 검색어를 담아 GET 요청을 하는데 이 페이지는 특이하게 키워드를 본문에 담은 POST 요청을 보내고 있었습니다. 해당 요청에 있던 본문을 그대로 복사해 Python으로 요청해봤습니다. 그런데 크롤링 제한이라고 써진 페이지를 응답으로 받았습니다. 문제점을 찾아보니 헤더까지 복사해 요청을 보내니 제대로된 결과를 받을 수 있었습니다. 아무래도 쿠키를 이용해 세션을 유지하면서 동시에 이를 이용해 크롤링 여부를 판단하는 것 같았습니다. 그래서 바로 데이터를 요청하는게 아니라 먼저 검색 페이지에 요청을 보내 `Set-Cookie` 헤더를 가져와 파싱해서 쿠키를 가져온 다음 이를 `Cookie` 헤더에 다시 담아 보내는 방식으로 요청을 보내봤습니다. 그런데 여전히 크롤링 제한 응답을 받았습니다. 왜 그런가 싶어서 다시 요청을 비교해보니, 브라우저의 요청에는 `Set-Cookie` 에 없던 쿠키가 추가되어 있었습니다. 문제를 해결하기 위해 페이지의 소스 코드를 열어보았습니다. 코드는 많았지만 대부분의 코드는 jQuery 번들러 코드였고, 해당 페이지에서 직접 사용하는 소스 코드는 많지 않았습니다. 해당 소스 코드에서 쿠키명을 검색하니 페이지 html 코드에서 특정 요소에 담긴 값을 해당 쿠키명으로 쿠키에 추가하는 코드가 있었습니다. 서버에서 레이아웃을 보낼 때, 이 숨겨진 요소를 담은 뒤 해당 요소를 브라우저에서 쿠키로 추가하는 방식으로 크롤링을 방지하고 있었던 것으로 이해했습니다. 그래서 마찬가지로 처음 검색 페이지 응답에서 html 코드도 파싱해 해당 요소에서 숨겨진 쿠키값을 찾아낸 뒤 쿠키로 추가해 크롤링 방지 기법을 우회하고 성공적으로 원하는 값만 빠르게 가져올 수 있도록 프로그램을 완성했습니다.
2. 크롤링을 방지하는 요소들이 많았을 텐데, 이를 우회하기 위해 어떤 기법을 사용했나요?
   1번의 대답으로 대체
3. 크롤러가 정확한 데이터를 수집할 수 있도록 어떤 검증 과정을 거쳤나요?
   필요한 자료만 올라와있기 때문에 추가적인 검증이 필요하지 않았습니다.
4. 구현한 크롤러의 성능을 최적화하기 위해 어떤 기술적 접근을 하셨나요?
   최대 수백건의 페이지를 크롤링해야 했고, 사이트 자체의 속도도 느리다 보니 이전 요청 응답을 받은 뒤 다시 요청을 하는 방식으로 크롤링을 하면 속도가 느릴 수 밖에 없었습니다. 파이썬의 `concurrent.futures.ThreadPoolExecutor` 로 병렬 처리 방식으로 바꾸니 소요시간이 최대 2.5배 증가해 훨씬 효율적인 코드를 작성할 수 있었습니다.
5. 구현한 크롤러는 얼마나 많은 데이터를 처리할 수 있으며, 시간당 처리량이나 성공률은 어떻게 되나요?
   약 150건의 페이지를 긁어와 엑셀로 저장하는데 2분이 채 걸리지 않았습니다. 성공률 또한 높아 개발 과정에서는 수백건의 요청을 거친 후에도 에러는 한 건 있을까 말까 한 정도였습니다.
6. Google Colab을 선택한 이유는 무엇인가요? 다른 배포 방법과 비교했을 때 Google Colab의 장점은 무엇이었나요?
   외주를 맡긴 분의 회사에서는 보안을 중요시 여겼기에 사내망에서 외부 프로그램을 실행하기가 많이 힘들었습니다. Google Colab은 실제 코드가 구글 서버에서 돌아가기 때문에 클라이언트 컴퓨터에서도 브라우저만 열 수 있으면 큰 문제 없이 실행할 수 있었습니다.
7. 이 프로젝트에서 Python을 선택하신 이유는 무엇인가요? Node.js와 비교했을 때 어떤 차이점이 있었나요?
   플랫폼이 Python 과 R 만을 지원했기 때문에 제게 더 익숙한 Python 을 사용했습니다.
8. 이 크롤러 프로젝트가 비즈니스적으로 어떤 영향을 미쳤나요?
   외주로 맡게된 프로그램이지만 맡기셨던 분의 후기를 들어보면 자료를 긁어오는 작업만 매주 8명이 인당 1시간 이상 걸리는 작업이었다고 했습니다. 제가 작성한 프로그램은 사용자가 할 일은 코드를 실행시키고 파일만 다운 받으면 되니 사실상 사람이 하는 일은 키 3개를 누르는 게 전부였습니다. 그럼 일주일에 최소 8시간은 아낀거라고 생각해도 될 것 같습니다.
9. 크롤링 방지 로직이 업데이트되면 이를 어떻게 대처하고 유지 보수할 계획인가요?
   외주로 진행한 작업이다보니 능동적인 대처나 유지 보수는 쉽진 않을 것 같습니다. 하지만 사용자의 에러 보고가 발생한다면 적극적으로 문제 해결을 위해 노력하겠습니다. 기존의 로직과 비교하며 변경 사항을 확인하고 유연하게 대처할 수 있도록 업데이트하겠습니다.   
10. 이 프로젝트를 통해 가장 어려웠던 점은 무엇이었고, 이를 어떻게 해결했나요?
    페이지 내부에 있는 숨겨진 요소에서 추가되는 쿠키값을 찾아내는 것이었습니다. 해결법은 1번 대답으로 대체하겠습니다.